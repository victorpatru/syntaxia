<context>
# Overview  
This PRD defines an MVP for a voice-first (not voice-only), engineer-led technical phone screen preparation product for senior software engineers (mid coaching mode available). The experience simulates a realistic technical phone screen via an AI interviewer using ElevenLabs for TTS and a modern STT engine. It emphasizes high-signal evaluation modes that work well over voice: code comprehension and debugging, pragmatic design/scenario Q&A, and concise background calibration. Code/logs/diffs are rendered on-screen while the agent speaks; a voice-only fallback uses structured summaries and on-demand readouts of small blocks.

The product serves candidates who want targeted practice aligned to real job descriptions (JDs) and fast, actionable feedback. Over time, it can expand to B2B team calibration modes, richer analytics, and editor-based coding.

# Core Features  
- **JD-conditioned session setup**  
  - Paste a JD, generate session conditioned to senior-level expectations and the JD's competencies.  
  - All content calibrated to senior full-stack engineer bar by default.

- **Voice technical phone screen (30–40 minutes)**  
  - A: Background depth warm-up (≈5m).  
  - B: Code comprehension & debugging (≈15–20m) on 2–3 short snippets.  
  - C: Scenario Q&A (design-lite) tied to the JD (≈10–15m).  
  - D: Wrap-up & immediate feedback (≈2–5m).

- **On-screen content rendering (voice-first UX)**  
  - Show code snippets, diffs, and logs in the UI while the agent speaks.  
  - Agent references line ranges/blocks; candidate can request highlights or zoom.  
  - Voice-only fallback: semantic summaries + on-demand reading of ≤25-line blocks.

- **Rubric-based scoring & feedback**  
  - Dimensions: Communication/clarity, Debugging/diagnosis, Breadth/trade-offs, Operationalization, Product sense.  
  - Anchored examples for 1–4 scale per dimension.

- **Transcript & report**  
  - Full transcript with highlighted moments, rubric scores, and a prioritized action plan.  
  - 3 tailored follow-up practice prompts.

 - **Full-Stack Content Focus (MVP)**  
  - Targeted at senior full-stack engineers: 8–12 curated snippets covering core full-stack competencies.  
  - Areas: auth/session & RBAC, API errors/pagination, caching strategies, rate limiting, async queues/retries, SQL correctness/transactions, logging/metrics/observability, feature flags, and background jobs.  
  - All snippets are 60–120 LOC with metadata (common risks, debugging hints, testability considerations).  
  - Future expansion: specialized packs based on user feedback and demand validation.

# User Experience  
- **Primary Persona**  
  - Senior engineers and ambitious mid-level engineers preparing for senior-level technical screens.  
  - All content assumes senior-level expectations - no dumbing down.  
  - Secondary (future): hiring teams calibrating interviewers.

- **Key Flows**  
  1) JD input → senior-level session preview.  
  2) Voice interview with turn-taking, interruptions (barge-in) supported.  
  3) Post-session report with scores, transcript, improvement bullets, and next practice prompts.  
  4) Optional: save JD profile and preferred domain tracks for repeat practice.

- **UX Considerations**  
  - Set expectations: “Not live coding—focus on debugging/design-lite.”  
  - Clear section transitions and timeboxing.  
  - Offer brief summaries and clarify ambiguous audio ("Did you say…?").
  - Voice-first with screen: content appears in-app; agent can trigger UI updates via tools.
</context>

<PRD>
# Technical Architecture  
- **System Components**  
  - Web App (TanStack Start): session creation, voice UI, results; uses Start’s streaming SSR.  
  - Convex backend: server functions for session orchestration, persistence, secure tool/webhook endpoints.  
  - Voice Agent Service: orchestrates TTS (ElevenLabs), STT (Whisper/Deepgram), LLM prompting, state machine for interview flow.  
  - Content Service: JD parser, domain pack selector, snippet retrieval, question banks.  
  - Reporting Service: rubric scorer, transcript analyzer, report generator (PDF/Markdown/JSON).  
  - Storage (Convex tables): sessions, transcripts, scores, JD profiles, snippet metadata.

- **Data Models (MVP)**  
  - `Session { id, jdSummary, level, domainTrack, sections[], startedAt, endedAt }`  
  - `Section { type (A|B|C|D), prompts[], responses[], scores? }`  
  - `Snippet { id, domain, code, risks[], tests? }`  
  - `Score { dimension, value (1-4), anchors, comment }`  
  - `TranscriptSegment { t, speaker, text }`

- **APIs & Integrations**  
  - ElevenLabs ConvAI WebSocket: low-latency voice; text events for UI; signed URLs for private agents.  
  - ElevenLabs JS SDK (`@elevenlabs/elevenlabs-js`): official SDK with conversation management, agent APIs, and client tools integration.  
  - ElevenLabs React SDK (`@elevenlabs/react`): official React hooks including `useConversation` for WebSocket/WebRTC connections, transcripts, and speaking state.  
  - Agent-driven UI via tools: `clientTools` (e.g., `show_snippet`, `highlight_lines`) using ConvAI Tools API to trigger UI updates.  
  - STT: ElevenLabs native ASR engine with streaming partials and confirmation prompts.  
  - LLM: deterministic, rubric-driven prompts; minimal variance.  
  - Optional analytics: product analytics + feature flagging.

- **Infra & Runtime**  
  - TanStack Start app with streaming SSR; Convex for serverless data/functions.  
  - Node/TypeScript throughout; streaming I/O for voice and transcripts.  
  - Secure secrets in server env; PII-light transcripts by default.

# Development Roadmap  
## MVP Requirements (ship this first)
- **Session Setup**  
  - JD ingestion: extract full-stack competencies and calibrate to senior-level expectations.  
  - Generate session plan (A–D) with senior-calibrated prompts/snippets focused on full-stack engineering skills.  
  - Match snippets to JD requirements within full-stack scope, maintaining senior-level difficulty.

- **A. Background Depth Warm-up (≈5m)**  
  - Prompts: ownership, scale, metrics, constraints.  
  - Scoring (Communication; Product sense).  
  - Output: concise calibration notes for later follow-ups.

- **B. Code Comprehension & Debugging (≈15–20m)**  
  - Pull 2–3 curated snippets (60–120 LOC) from content packs matching the JD.  
  - Display snippet in the UI; agent references line ranges/blocks and asks follow-ups.  
  - Candidate can request highlights or read-backs of specific lines/blocks.  
  - Voice-only fallback: provide structured summaries (functions, branches, external calls) and on-demand reading of ≤25-line blocks or short diffs/logs.  
  - Agent pattern: summarize → ask for risks/bugs → probe fixes → ask test plan & edge cases.  
  - Scoring (Debugging & diagnosis; Operationalization; Communication).  
  - Output: identified issues, proposed minimal fix, test plan bullets.

- **C. Scenario Q&A (Design-lite) (≈10–15m)**  
  - Full-stack scenarios: auth flows, API design, database choices, caching strategies, monitoring/observability, deployment patterns.  
  - Agent pattern: start broad → constraint-based follow-ups → rollout/observability plan.  
  - Focus on practical trade-offs senior full-stack engineers face daily.  
  - Scoring (Breadth & trade-offs; Operationalization; Communication).  
  - Output: structured design outline with explicit trade-offs.

- **D. Wrap-up & Feedback (≈2–5m)**  
  - 3–5 concrete improvement bullets tied to rubric anchors.  
  - 3 tailored practice prompts for next session.  
  - Suggested reading/exercise per weak area.

- **Report Generation**  
  - Transcript with highlights (timestamps).  
  - Rubric (1–4) per dimension with short anchor-based comments.  
  - Action plan: prioritized improvements; follow-up prompts.  
  - Export: Markdown/PDF; JSON for programmatic review.

- **Voice UX**  
  - Barge-in enabled via ElevenLabs turn-taking and interrupt detection; concise prompts; section handoffs.  
  - STT confirmation for critical terms; medium TTS speed with streaming partials.  
  - Agent→UI control via `clientTools` (e.g., `show_snippet({id})`, `highlight_lines({from,to})`) using ConvAI Tools API.

- **Full-Stack Content Library (MVP scope)**  
  - 8–12 curated full-stack snippets with metadata (common risks, debugging hints, testability).  
  - 15–20 full-stack scenario seeds covering auth, APIs, databases, caching, monitoring.

## Future Enhancements
- Optional in-browser code runner with unit tests for a light coding task.  
- Interviewer personas (strict vs. coaching) and calibration mode.  
- B2B team mode with hiring rubrics and score normalization.  
- Scheduling, payments, multi-language voices/locales.  
- Rich analytics on question difficulty and improvement deltas.

# Logical Dependency Chain  
1. Foundations: TanStack Start app shell; Convex schema; session model; JD parser; senior-level calibration.  
2. Content: full-stack snippet library + scenario seeds with metadata.  
3. Voice agent: flow state machine (A–D), ConvAI WebSocket, streaming TTS/STT with native ASR, turn-taking and interrupt detection.  
4. Scoring: rubric engine + anchor comments.  
5. Reporting: transcript highlights, scores, action plan, export.  
6. JD conditioning: competency extraction → full-stack content matching → senior-level calibration.

# Risks and Mitigations  
- **STT accuracy**: use ElevenLabs native ASR with streaming partials and confirmation prompts for critical details; allow candidate repetition.  
- **Generic feel**: JD conditioning + full-stack focus; dynamic follow-ups keyed to answers.  
- **Voice monotony**: rotate snippets/scenarios; vary prompts; add brief summaries.  
- **Expectation mismatch (no live coding)**: set expectations upfront; later add optional code runner.  
- **UI sync & connectivity**: keep ConvAI/WebSocket alive (ping/pong); deterministic UI events via `clientTools` with idempotent Convex handlers.  
- **React/Client tools integration**: ensure client tools (UI control) work seamlessly with React SDK state management and `useConversation` hook.

# Appendix  
**Rubric Anchors (1–4)**  
- Communication & clarity: 1 vague → 4 structured, precise, avoids hand-waving.  
- Debugging & diagnosis: 1 superficial → 4 root-cause minimal fix + testability.  
- Breadth & trade-offs: 1 unconstrained → 4 constraints-first, explicit trade-offs.  
- Operationalization: 1 ignores ops → 4 rollout/observability/failure modes.  
- Product sense: 1 no impact → 4 impact and user considerations where relevant.

**Example JD-Calibrated Prompts**  
- Arcjet (Web/DX): “Identify session risks in this middleware; propose a minimal, testable fix. How would you roll out a new rate limiter safely?”  
- Railway (GraphQL/Temporal): “Where do resolver boundaries end vs. workflow activities? Ensure idempotency and retries without duplicating side effects.”  
- PostHog (Analytics): “Design an ingestion path with experimentation guardrails and backfills. What metrics/alerts/dashboards detect regressions?”  
- OpenRouter (Edge/Workers): “Design a rate limiter at the edge; discuss caching, token bucket placement, and latency/cost trade-offs. Observability plan?”

**Success Criteria (MVP Validation)**  
- 10–20 paid sessions from senior candidates within 2 weeks (mid via coaching mode).  
- CSAT ≥ 4.5/5; ≥ 40% intent to repurchase for new JD/company.  
- Top themes: “felt realistic” and “I know what to fix now.”
</PRD>

